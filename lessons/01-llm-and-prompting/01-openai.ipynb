{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI - the major LLM provider\n",
    "\n",
    "## Introduction\n",
    "[OpenAI](https://openai.com/) is the company behind Chatgpt and is the most popular Large Language Model (LLM) provider. It is offered as a SaaS solution. Azure OpenAI from Microsoft has an offering that provides the same model but within an Azure Environment.\n",
    "\n",
    "When OpenAI initally launched they offered an LLM model called `gpt-3`\n",
    "\n",
    "## Model types\n",
    "Now they provide many different models such as : <https://platform.openai.com/docs/models>\n",
    "- `gpt-4-turbo` is a large multimodal model (accepting text or image inputs and outputting text) that can solve difficult problems with greater accuracy than any of our previous models, thanks to its broader general knowledge and advanced reasoning capabilitie   \n",
    "- `gpt-4o` (“o” for “omni”) is our most advanced model. It is multimodal (accepting text or image inputs and outputting text), and it has the same high intelligence as GPT-4 Turbo but is much more efficient—it generates text 2x faster and is 50% cheaper\n",
    "- `gpt-4o-mini` (“o” for “omni”) is our most advanced model in the small models category, and our cheapest model yet. It is multimodal (accepting text or image inputs and outputting text), has higher intelligence than gpt-3.5-turbo but is just as fast. It is meant to be used for smaller tasks, including vision tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "OpenAI has an SDK to call their model. Let's install it the python SDK first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple example using the SDK \n",
    "\n",
    "Now we use the api to do a simple api call and get the result back.\n",
    "\n",
    "The result is called a `completion` as llms are in essence auto-completion systems on steriods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have it write a Haiku about DevOps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-B9wA9lHYZSSKMuQdBecbmz0CCGjKr', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Code flows like a stream,  \\nGenAI weaves through DevOps,  \\nDreams build and deploy.', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1741708229, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier='default', system_fingerprint='fp_06737a9306', usage=CompletionUsage(completion_tokens=23, prompt_tokens=28, total_tokens=51, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n"
     ]
    }
   ],
   "source": [
    "# We import the library\n",
    "from openai import OpenAI\n",
    "\n",
    "# We instantiate an OpenAI client\n",
    "client = OpenAI()\n",
    "\n",
    "# We select one of the models\n",
    "# gpt_model = \"gpt-3.5-turbo\"\n",
    "# gpt_model = \"gpt-4o\"\n",
    "# gpt_model = \"gpt-4o-turbo\"\n",
    "model = \"gpt-4o-mini\"\n",
    "\n",
    "# The question we're going to ask\n",
    "question =  \"Write a haiku about genAI and DevOps.\"\n",
    "\n",
    "# Next we create the completion\n",
    "completion = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": question\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "# And finally we print the result\n",
    "print(completion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracing the right information\n",
    "The text response is nexted in the response.\n",
    "\n",
    "To extract the response from the completion, we find the relevant element in the completion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code flows like a stream,  \n",
      "GenAI weaves through DevOps,  \n",
      "Dreams build and deploy.\n"
     ]
    }
   ],
   "source": [
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations you have now succesfully called the OpenAI llm. Feel free to experiment with different models. Or ask your own questions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
