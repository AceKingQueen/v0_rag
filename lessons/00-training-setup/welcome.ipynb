{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GenAI Workshop Test lesson \n",
    "This Jupyter notebook will help you test if you're setup for the genAI DevOps workshop.\n",
    "\n",
    "Execute the step by pressing the play button in front of the code blocks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the openAI API key first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "openai_api_key = os.environ.get('OPENAI_API_KEY')\n",
    "assert openai_api_key, 'Please set the OPENAI_API_KEY environment variable in your codespace secret settings'\n",
    "assert openai_api_key.startswith('sk-'), 'Invalid OPENAI_API_KEY'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the Anthropic API key next. No worries if this fails, this is optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "anthropic_api_key = os.environ.get('ANTHROPIC_API_KEY')\n",
    "assert anthropic_api_key, 'Please set the ANTHROPIC_API_KEY environment variable in your codespace secret settings'\n",
    "assert anthropic_api_key.startswith('sk-'), 'Invalid ANTHROPIC_API_KEY'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now test to see if we can install some python packages using `pip` the python package manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q openai anthropic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run a simple code example usingOpenAI's GPT-4o-mini model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-B9w1HaQKLuj9GrErzWoGkDExkgBXl', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='RAG can refer to several different concepts depending on the context. Here are some of the most common meanings:\\n\\n1. **RAG Rating (Red, Amber, Green Status):** This is a project management technique used to indicate the status or performance of a project or task. Red indicates problems, Amber indicates caution or warning, and Green indicates everything is fine.\\n\\n2. **Retrieval-Augmented Generation (RAG):** This is a concept in natural language processing where a model combines a retrieval system with a generation system. It retrieves relevant documents from a large corpus based on a query and uses them to enhance or inform the generation of responses. This approach is particularly useful for tasks that benefit from accessing external information to generate more accurate or contextually relevant outputs.\\n\\n3. **Recombination Activating Genes (RAG):** In immunology, RAG-1 and RAG-2 are critical components in the development of immune diversity. They are involved in the rearrangement of gene segments that generate diverse antibody and T-cell receptor repertoires.\\n\\nIf you provide more context, I can give a more precise explanation tailored to your needs!', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1741707679, model='gpt-4o-2024-08-06', object='chat.completion', service_tier='default', system_fingerprint='fp_eb9dce56a8', usage=CompletionUsage(completion_tokens=231, prompt_tokens=21, total_tokens=252, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "model = \"gpt-4o\"\n",
    "\n",
    "question =  \"What is RAG\"\n",
    "completion = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": question\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(completion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now just get message only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG can refer to several different concepts depending on the context. Here are some of the most common meanings:\n",
      "\n",
      "1. **RAG Rating (Red, Amber, Green Status):** This is a project management technique used to indicate the status or performance of a project or task. Red indicates problems, Amber indicates caution or warning, and Green indicates everything is fine.\n",
      "\n",
      "2. **Retrieval-Augmented Generation (RAG):** This is a concept in natural language processing where a model combines a retrieval system with a generation system. It retrieves relevant documents from a large corpus based on a query and uses them to enhance or inform the generation of responses. This approach is particularly useful for tasks that benefit from accessing external information to generate more accurate or contextually relevant outputs.\n",
      "\n",
      "3. **Recombination Activating Genes (RAG):** In immunology, RAG-1 and RAG-2 are critical components in the development of immune diversity. They are involved in the rearrangement of gene segments that generate diverse antibody and T-cell receptor repertoires.\n",
      "\n",
      "If you provide more context, I can give a more precise explanation tailored to your needs!\n"
     ]
    }
   ],
   "source": [
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message(id='msg_012aAqWpE1K2Sox6S7wLC5bX', content=[TextBlock(citations=None, text='# Retrieval-Augmented Generation (RAG)\\n\\nRAG, or Retrieval-Augmented Generation, is a framework that enhances large language models (LLMs) by combining them with a retrieval system to access external knowledge.\\n\\n## How RAG Works:\\n\\n1. **Retrieval**: When a query is received, RAG searches a knowledge base (documents, databases, etc.) for relevant information.\\n\\n2. **Augmentation**: The retrieved information is added to the prompt sent to the language model.\\n\\n3. **Generation**: The language model generates a response using both the original query and the retrieved context.\\n\\n## Key Benefits:\\n\\n- **Up-to-date information**: Overcomes the knowledge cutoff limitations of pre-trained LLMs\\n- **Better factual accuracy**: Reduces hallucinations by grounding responses in retrieved facts\\n- **Domain specialization**: Can be customized with specific knowledge bases for particular fields\\n- **Transparency**: Retrieved sources can be cited to support generated responses\\n\\nRAG is widely used in applications like question-answering systems, chatbots, and enterprise search tools where accurate, current information is crucial.', type='text')], model='claude-3-7-sonnet-20250219', role='assistant', stop_reason='end_turn', stop_sequence=None, type='message', usage=Usage(cache_creation_input_tokens=0, cache_read_input_tokens=0, input_tokens=17, output_tokens=253))\n",
      "\n",
      "Claude's response:\n",
      "# Retrieval-Augmented Generation (RAG)\n",
      "\n",
      "RAG, or Retrieval-Augmented Generation, is a framework that enhances large language models (LLMs) by combining them with a retrieval system to access external knowledge.\n",
      "\n",
      "## How RAG Works:\n",
      "\n",
      "1. **Retrieval**: When a query is received, RAG searches a knowledge base (documents, databases, etc.) for relevant information.\n",
      "\n",
      "2. **Augmentation**: The retrieved information is added to the prompt sent to the language model.\n",
      "\n",
      "3. **Generation**: The language model generates a response using both the original query and the retrieved context.\n",
      "\n",
      "## Key Benefits:\n",
      "\n",
      "- **Up-to-date information**: Overcomes the knowledge cutoff limitations of pre-trained LLMs\n",
      "- **Better factual accuracy**: Reduces hallucinations by grounding responses in retrieved facts\n",
      "- **Domain specialization**: Can be customized with specific knowledge bases for particular fields\n",
      "- **Transparency**: Retrieved sources can be cited to support generated responses\n",
      "\n",
      "RAG is widely used in applications like question-answering systems, chatbots, and enterprise search tools where accurate, current information is crucial.\n"
     ]
    }
   ],
   "source": [
    "from anthropic import Anthropic\n",
    "\n",
    "# Initialize the Anthropic client with your API key\n",
    "client = Anthropic()\n",
    "\n",
    "# Define the model to use\n",
    "model = \"claude-3-7-sonnet-20250219\"  # This is the model string for Claude 3.7 Sonnet\n",
    "\n",
    "# Define your question\n",
    "question = \"What is RAG\"\n",
    "\n",
    "# Create a message with Claude\n",
    "response = client.messages.create(\n",
    "    model=model,\n",
    "    max_tokens=1000,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": question\n",
    "        }\n",
    "    ],\n",
    "    system=\"You are a helpful assistant.\"\n",
    ")\n",
    "\n",
    "# Print the full response\n",
    "print(response)\n",
    "\n",
    "# Print just the text response\n",
    "print(\"\\nClaude's response:\")\n",
    "print(response.content[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations ! You're all set for the workshop. \n",
    "\n",
    "Don't forget to send us your github user so we can share the code of the actual workshop !"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
